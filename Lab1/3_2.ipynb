{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "3.2 Classification and regression with a two-layer perceptron\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generate some data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import trange\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "def generate_data(mA=(1, 1), mB=(5, 1), sigmaA=1, sigmaB=1, type='random'):\n",
    "    np.random.seed(100)\n",
    "    A = np.random.randn(256, 2) * sigmaA + mA\n",
    "    B = np.random.randn(256, 2) * sigmaB + mB\n",
    "    labels = np.concatenate((np.ones(A.shape[0]), -np.ones(B.shape[0])))\n",
    "    samples = np.concatenate((A, B))\n",
    "    permute = np.random.permutation(A.shape[0] + B.shape[0])\n",
    "    return samples[permute, :], labels[permute]\n",
    "\n",
    "samples,labels = generate_data()\n",
    "for i, it in enumerate(samples):\n",
    "    if labels[i] == -1:\n",
    "        plt.plot(samples[i][0], samples[i][1], 'r.')\n",
    "    else:\n",
    "        plt.plot(samples[i][0], samples[i][1], 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Def of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(x):\n",
    "#     return 2 / (1 + np.exp(-x))-1\n",
    "\n",
    "# def sigmoid_derivative(x):\t\n",
    "#     return (1+sigmoid(x))* (1 - sigmoid(x))/2\n",
    "\n",
    "# class NN:\n",
    "#     def __init__(self,num_nodes=[2,4,4,1]):\n",
    "#         self.weights= []\n",
    "#         self.activation = sigmoid\n",
    "#         self.activation_deriv = sigmoid_derivative\n",
    "#         self.num_nodes = num_nodes\n",
    "#         self.bias=[]\n",
    "#         self.hout_batch=[]\n",
    "#         for i in range(1,len(num_nodes)):\n",
    "#             self.weights.append(np.random.randn(num_nodes[i-1],num_nodes[i]))\n",
    "#             self.bias.append(np.random.randn(num_nodes[i]))\n",
    "                \n",
    "#         return \n",
    "#     def forward_pass(self,x):\n",
    "#         result = x\n",
    "#         hout= []\n",
    "#         for i,matrix in enumerate(self.weights):\n",
    "#             result=self.activation(np.dot(result,matrix))+self.bias[i]\n",
    "#             hout.append(result)\n",
    "#         return result,hout\n",
    "#     def train(self,X,Y,learning_rate = 0.4, epochs=5):\n",
    "#         n = len(Y)\n",
    "#         for k in trange(epochs):\n",
    "#             learning_rate*=0.5\n",
    "#             for x,y in zip(X,Y):\n",
    "#                 result,hout = self.forward_pass(x)\n",
    "#                 deltas =[(hout[-1]-y)*self.activation_deriv(result)).item()]\n",
    "\n",
    "#                 layer_num = len(hout)-2\n",
    "#                 for j in range(layer_num,0,-1):\n",
    "#                     deltas.append(np.dot(deltas[-1],self.weights[j].T)* self.activation_deriv(hout[j]))\n",
    "#                 deltas.reverse()\n",
    "#                 for i in range(len(self.weights)):\n",
    "#                     layer = np.atleast_2d(hout[i])\n",
    "#                     delta = np.atleast_2d(deltas[i])\n",
    "#                     self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "#                     self.bias[i] += learning_rate * deltas[i]\n",
    "#         return \n",
    "#     def show(self):\n",
    "#         for i in self.weights:\n",
    "#             print(i.shape)\n",
    "\n",
    "#     def predict(self,x):\n",
    "#         result=x\n",
    "#         for i,matrix in enumerate(self.weights):\n",
    "#             result=self.activation(np.dot(result,matrix))+self.bias[i]\n",
    "#         return np.sign(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Parameter(object):\n",
    "    def __init__(self, data, requires_grad, skip_decay=False):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.skip_decay = skip_decay\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "    @property\n",
    "    def T(self):\n",
    "        return self.data.T\n",
    "\n",
    "class SGD(object):\n",
    "    def __init__(self, parameters, lr, decay=0):\n",
    "        self.parameters = [p for p in parameters if p.requires_grad]   \n",
    "        self.lr = lr\n",
    "        self.decay_rate = 1.0 - decay\n",
    "\n",
    "    def update(self):\n",
    "        for p in self.parameters:\n",
    "            if self.decay_rate < 1 and not p.skip_decay: p.data *= self.decay_rate\n",
    "           # print(\"data\",p.data,\"grad\",p.grad)\n",
    "            p.data -= self.lr * p.grad\n",
    "\n",
    "\n",
    "class MSE:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def gradient(self):\n",
    "        return self.a - self.y\n",
    "    \n",
    "    def __call__(self, output, target, requires_acc=True):\n",
    "        self.a = output\n",
    "        self.y = np.reshape(target,(-1,1))\n",
    "        loss = 0.5*np.multiply(self.a-self.y,self.a-self.y).mean()\n",
    "        if requires_acc:\n",
    "            acc = np.sum(np.sign(output)==self.y)/output.shape[0]\n",
    "            return loss,acc\n",
    "        return loss\n",
    "\n",
    "class Layer():\n",
    "    @abstractmethod\n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, *args):\n",
    "        pass\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def forward(self, x):\n",
    "        ex = np.exp(x)\n",
    "        esx = np.exp(-x)\n",
    "        self.y = (ex - esx) / (ex + esx)\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, eta):\n",
    "        return np.einsum('...,...,...->...', 1 - self.y, 1 + self.y, eta, optimize=True)\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, shape, requires_grad=True, bias=True, **kwargs):\n",
    "        '''\n",
    "        shape = (in_size, out_size)\n",
    "        '''\n",
    "        W = np.random.randn(*shape) * (2 / shape[0]**0.5)\n",
    "      #  print(W.shape)\n",
    "        self.W = Parameter(W, requires_grad)\n",
    "        self.b = Parameter(np.zeros(shape[-1]), requires_grad) if bias else None\n",
    "        self.require_grad = requires_grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.require_grad: self.x = x\n",
    "        out = np.dot(x, self.W.data)\n",
    "        if self.b is not None: out = out + self.b.data\n",
    "        return out\n",
    "\n",
    "    def backward(self, eta):\n",
    "       if self.require_grad:\n",
    "            batch_size = eta.shape[0]\n",
    "            self.W.grad = np.dot(self.x.T, eta) / batch_size\n",
    "            if self.b is not None: self.b.grad = np.sum(eta, axis=0) / batch_size\n",
    "       return np.dot(eta, self.W.T)\n",
    "class Net(Layer):\n",
    "    def __init__(self, layer_configures):\n",
    "        self.layers = []\n",
    "        self.parameters=[]\n",
    "        for config in layer_configures:\n",
    "            self.layers.append(self.createLayer(config))\n",
    "\n",
    "    def createLayer(self, config):\n",
    "        return self.getDefaultLayer(config)\n",
    "\n",
    "    def getDefaultLayer(self, config):\n",
    "        t = config['type']\n",
    "        if t == 'linear':\n",
    "            layer = Linear(**config)\n",
    "            self.parameters.append(layer.W)\n",
    "            if layer.b is not None: self.parameters.append(layer.b)\n",
    "        elif t == 'tanh':\n",
    "            layer = Tanh()\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, eta):\n",
    "        for layer in self.layers[::-1]:\n",
    "            eta = layer.backward(eta)\n",
    "        return eta\n",
    "    \n",
    "    def train(self,X,Y,optimizer,batch_size=16,epochs=500,loss=MSE()):\n",
    "        n = len(Y)\n",
    "        for epoch in trange(epochs):\n",
    "            i = 0 \n",
    "            while i<= n-batch_size:\n",
    "                x,y=X[i:i+batch_size,],Y[i:i+batch_size,]\n",
    "                i+=batch_size\n",
    "                output = self.forward(x)\n",
    "                batch_loss, batch_acc = loss(output,y)\n",
    "                eta = loss.gradient()\n",
    "                self.backward(eta)\n",
    "                optimizer.update()\n",
    "                if epoch % 40:\n",
    "                    print(\"epoch: %d, batch: %5d, batch_acc:    %.2f,batch loss: %.2f\" % \\\n",
    "                    (epoch, i/batch_size,batch_acc*100,batch_loss))\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nepoch: 128, batch:    25, batch_acc:    93.75,batch loss: 0.11\nepoch: 128, batch:    26, batch_acc:    100.00,batch loss: 0.04\nepoch: 128, batch:    27, batch_acc:    100.00,batch loss: 0.03\nepoch: 128, batch:    28, batch_acc:    100.00,batch loss: 0.07\nepoch: 128, batch:    29, batch_acc:    100.00,batch loss: 0.06\nepoch: 128, batch:    30, batch_acc:    87.50,batch loss: 0.16\nepoch: 128, batch:    31, batch_acc:    93.75,batch loss: 0.09\nepoch: 128, batch:    32, batch_acc:    93.75,batch loss: 0.09\nepoch: 129, batch:     1, batch_acc:    87.50,batch loss: 0.15\nepoch: 129, batch:     2, batch_acc:    100.00,batch loss: 0.02\nepoch: 129, batch:     3, batch_acc:    93.75,batch loss: 0.08\nepoch: 129, batch:     4, batch_acc:    100.00,batch loss: 0.06\nepoch: 129, batch:     5, batch_acc:    81.25,batch loss: 0.26\nepoch: 129, batch:     6, batch_acc:    93.75,batch loss: 0.11\nepoch: 129, batch:     7, batch_acc:    87.50,batch loss: 0.19\nepoch: 129, batch:     8, batch_acc:    93.75,batch loss: 0.10\nepoch: 129, batch:     9, batch_acc:    100.00,batch loss: 0.10\nepoch: 129, batch:    10, batch_acc:    93.75,batch loss: 0.13\nepoch: 129, batch:    11, batch_acc:    93.75,batch loss: 0.08\nepoch: 129, batch:    12, batch_acc:    100.00,batch loss: 0.03\nepoch: 129, batch:    13, batch_acc:    93.75,batch loss: 0.12\nepoch: 129, batch:    14, batch_acc:    87.50,batch loss: 0.13\nepoch: 129, batch:    15, batch_acc:    93.75,batch loss: 0.10\nepoch: 129, batch:    16, batch_acc:    93.75,batch loss: 0.18\nepoch: 129, batch:    17, batch_acc:    100.00,batch loss: 0.06\nepoch: 129, batch:    18, batch_acc:    93.75,batch loss: 0.10\nepoch: 129, batch:    19, batch_acc:    75.00,batch loss: 0.30\nepoch: 129, batch:    20, batch_acc:    100.00,batch loss: 0.05\nepoch: 129, batch:    21, batch_acc:    93.75,batch loss: 0.10\nepoch: 129, batch:    22, batch_acc:    87.50,batch loss: 0.15\nepoch: 129, batch:    23, batch_acc:    100.00,batch loss: 0.06\nepoch: 129, batch:    24, batch_acc:    93.75,batch loss: 0.09\nepoch: 129, batch:    25, batch_acc:    93.75,batch loss: 0.11\nepoch: 129, batch:    26, batch_acc:    100.00,batch loss: 0.03\nepoch: 129, batch:    27, batch_acc:    100.00,batch loss: 0.03\nepoch: 129, batch:    28, batch_acc:    100.00,batch loss: 0.07\nepoch: 129, batch:    29, batch_acc:    100.00,batch loss: 0.06\nepoch: 129, batch:    30, batch_acc:    87.50,batch loss: 0.16\nepoch: 129, batch:    31, batch_acc:    93.75,batch loss: 0.09\nepoch: 129, batch:    32, batch_acc:    93.75,batch loss: 0.09\n 26%|██▌       | 130/500 [00:13<00:38,  9.61it/s]epoch: 130, batch:     1, batch_acc:    87.50,batch loss: 0.15\nepoch: 130, batch:     2, batch_acc:    100.00,batch loss: 0.02\nepoch: 130, batch:     3, batch_acc:    93.75,batch loss: 0.08\nepoch: 130, batch:     4, batch_acc:    100.00,batch loss: 0.06\nepoch: 130, batch:     5, batch_acc:    81.25,batch loss: 0.26\nepoch: 130, batch:     6, batch_acc:    93.75,batch loss: 0.11\nepoch: 130, batch:     7, batch_acc:    87.50,batch loss: 0.19\nepoch: 130, batch:     8, batch_acc:    93.75,batch loss: 0.10\nepoch: 130, batch:     9, batch_acc:    100.00,batch loss: 0.09\nepoch: 130, batch:    10, batch_acc:    93.75,batch loss: 0.13\nepoch: 130, batch:    11, batch_acc:    93.75,batch loss: 0.08\nepoch: 130, batch:    12, batch_acc:    100.00,batch loss: 0.03\nepoch: 130, batch:    13, batch_acc:    93.75,batch loss: 0.12\nepoch: 130, batch:    14, batch_acc:    87.50,batch loss: 0.13\nepoch: 130, batch:    15, batch_acc:    93.75,batch loss: 0.10\nepoch: 130, batch:    16, batch_acc:    93.75,batch loss: 0.18\nepoch: 130, batch:    17, batch_acc:    100.00,batch loss: 0.06\nepoch: 130, batch:    18, batch_acc:    93.75,batch loss: 0.10\nepoch: 130, batch:    19, batch_acc:    75.00,batch loss: 0.30\nepoch: 130, batch:    20, batch_acc:    100.00,batch loss: 0.05\nepoch: 130, batch:    21, batch_acc:    93.75,batch loss: 0.10\nepoch: 130, batch:    22, batch_acc:    87.50,batch loss: 0.15\nepoch: 130, batch:    23, batch_acc:    100.00,batch loss: 0.06\nepoch: 130, batch:    24, batch_acc:    93.75,batch loss: 0.08\nepoch: 130, batch:    25, batch_acc:    93.75,batch loss: 0.11\nepoch: 130, batch:    26, batch_acc:    100.00,batch loss: 0.03\nepoch: 130, batch:    27, batch_acc:    100.00,batch loss: 0.03\nepoch: 130, batch:    28, batch_acc:    100.00,batch loss: 0.06\nepoch: 130, batch:    29, batch_acc:    100.00,batch loss: 0.06\nepoch: 130, batch:    30, batch_acc:    87.50,batch loss: 0.16\nepoch: 130, batch:    31, batch_acc:    93.75,batch loss: 0.08\nepoch: 130, batch:    32, batch_acc:    93.75,batch loss: 0.09\nepoch: 131, batch:     1, batch_acc:    87.50,batch loss: 0.15\nepoch: 131, batch:     2, batch_acc:    100.00,batch loss: 0.02\nepoch: 131, batch:     3, batch_acc:    93.75,batch loss: 0.08\nepoch: 131, batch:     4, batch_acc:    100.00,batch loss: 0.06\nepoch: 131, batch:     5, batch_acc:    81.25,batch loss: 0.26\nepoch: 131, batch:     6, batch_acc:    93.75,batch loss: 0.11\nepoch: 131, batch:     7, batch_acc:    87.50,batch loss: 0.19\nepoch: 131, batch:     8, batch_acc:    93.75,batch loss: 0.10\nepoch: 131, batch:     9, batch_acc:    100.00,batch loss: 0.09\nepoch: 131, batch:    10, batch_acc:    93.75,batch loss: 0.13\nepoch: 131, batch:    11, batch_acc:    93.75,batch loss: 0.08\nepoch: 131, batch:    12, batch_acc:    100.00,batch loss: 0.03\nepoch: 131, batch:    13, batch_acc:    93.75,batch loss: 0.12\nepoch: 131, batch:    14, batch_acc:    93.75,batch loss: 0.13\nepoch: 131, batch:    15, batch_acc:    93.75,batch loss: 0.10\nepoch: 131, batch:    16, batch_acc:    93.75,batch loss: 0.18\nepoch: 131, batch:    17, batch_acc:    100.00,batch loss: 0.06\nepoch: 131, batch:    18, batch_acc:    93.75,batch loss: 0.10\nepoch: 131, batch:    19, batch_acc:    75.00,batch loss: 0.30\nepoch: 131, batch:    20, batch_acc:    100.00,batch loss: 0.05\nepoch: 131, batch:    21, batch_acc:    93.75,batch loss: 0.10\nepoch: 131, batch:    22, batch_acc:    87.50,batch loss: 0.15\nepoch: 131, batch:    23, batch_acc:    100.00,batch loss: 0.05\nepoch: 131, batch:    24, batch_acc:    93.75,batch loss: 0.08\nepoch: 131, batch:    25, batch_acc:    93.75,batch loss: 0.11\nepoch: 131, batch:    26, batch_acc:    100.00,batch loss: 0.03\nepoch: 131, batch:    27, batch_acc:    100.00,batch loss: 0.03\nepoch: 131, batch:    28, batch_acc:    100.00,batch loss: 0.06\nepoch: 131, batch:    29, batch_acc:    100.00,batch loss: 0.06\nepoch: 131, batch:    30, batch_acc:    87.50,batch loss: 0.16\nepoch: 131, batch:    31, batch_acc:    93.75,batch loss: 0.08\nepoch: 131, batch:    32, batch_acc:    93.75,batch loss: 0.08\n 26%|██▋       | 132/500 [00:13<00:40,  9.05it/s]epoch: 132, batch:     1, batch_acc:    87.50,batch loss: 0.15\nepoch: 132, batch:     2, batch_acc:    100.00,batch loss: 0.02\nepoch: 132, batch:     3, batch_acc:    93.75,batch loss: 0.08\nepoch: 132, batch:     4, batch_acc:    100.00,batch loss: 0.06\nepoch: 132, batch:     5, batch_acc:    81.25,batch loss: 0.26\nepoch: 132, batch:     6, batch_acc:    93.75,batch loss: 0.11\nepoch: 132, batch:     7, batch_acc:    87.50,batch loss: 0.19\nepoch: 132, batch:     8, batch_acc:    93.75,batch loss: 0.10\nepoch: 132, batch:     9, batch_acc:    100.00,batch loss: 0.09\nepoch: 132, batch:    10, batch_acc:    93.75,batch loss: 0.13\nepoch: 132, batch:    11, batch_acc:    93.75,batch loss: 0.08\nepoch: 132, batch:    12, batch_acc:    100.00,batch loss: 0.03\nepoch: 132, batch:    13, batch_acc:    93.75,batch loss: 0.12\nepoch: 132, batch:    14, batch_acc:    93.75,batch loss: 0.13\nepoch: 132, batch:    15, batch_acc:    93.75,batch loss: 0.10\nepoch: 132, batch:    16, batch_acc:    93.75,batch loss: 0.18\nepoch: 132, batch:    17, batch_acc:    100.00,batch loss: 0.06\nepoch: 132, batch:    18, batch_acc:    93.75,batch loss: 0.10\nepoch: 132, batch:    19, batch_acc:    75.00,batch loss: 0.29\nepoch: 132, batch:    20, batch_acc:    100.00,batch loss: 0.05\nepoch: 132, batch:    21, batch_acc:    93.75,batch loss: 0.10\nepoch: 132, batch:    22, batch_acc:    87.50,batch loss: 0.15\nepoch: 132, batch:    23, batch_acc:    100.00,batch loss: 0.05\nepoch: 132, batch:    24, batch_acc:    93.75,batch loss: 0.08\nepoch: 132, batch:    25, batch_acc:    93.75,batch loss: 0.11\nepoch: 132, batch:    26, batch_acc:    100.00,batch loss: 0.03\nepoch: 132, batch:    27, batch_acc:    100.00,batch loss: 0.03\nepoch: 132, batch:    28, batch_acc:    100.00,batch loss: 0.06\nepoch: 132, batch:    29, batch_acc:    100.00,batch loss: 0.06\nepoch: 132, batch:    30, batch_acc:    87.50,batch loss: 0.16\nepoch: 132, batch:    31, batch_acc:    93.75,batch loss: 0.08\nepoch: 132, batch:    32, batch_acc:    93.75,batch loss: 0.08\nepoch: 133, batch:     1, batch_acc:    87.50,batch loss: 0.15\nepoch: 133, batch:     2, batch_acc:    100.00,batch loss: 0.02\nepoch: 133, batch:     3, batch_acc:    93.75,batch loss: 0.08\nepoch: 133, batch:     4, batch_acc:    100.00,batch loss: 0.06\nepoch: 133, batch:     5, batch_acc:    81.25,batch loss: 0.26\nepoch: 133, batch:     6, batch_acc:    93.75,batch loss: 0.10\nepoch: 133, batch:     7, batch_acc:    87.50,batch loss: 0.19\nepoch: 133, batch:     8, batch_acc:    93.75,batch loss: 0.10\nepoch: 133, batch:     9, batch_acc:    100.00,batch loss: 0.09\nepoch: 133, batch:    10, batch_acc:    93.75,batch loss: 0.13\nepoch: 133, batch:    11, batch_acc:    93.75,batch loss: 0.08\nepoch: 133, batch:    12, batch_acc:    100.00,batch loss: 0.03\nepoch: 133, batch:    13, batch_acc:    93.75,batch loss: 0.12\nepoch: 133, batch:    14, batch_acc:    93.75,batch loss: 0.13\nepoch: 133, batch:    15, batch_acc:    93.75,batch loss: 0.10\nepoch: 133, batch:    16, batch_acc:    93.75,batch loss: 0.18\nepoch: 133, batch:    17, batch_acc:    100.00,batch loss: 0.06\nepoch: 133, batch:    18, batch_acc:    93.75,batch loss: 0.09\nepoch: 133, batch:    19, batch_acc:    75.00,batch loss: 0.29\nepoch: 133, batch:    20, batch_acc:    100.00,batch loss: 0.05\nepoch: 133, batch:    21, batch_acc:    93.75,batch loss: 0.10\nepoch: 133, batch:    22, batch_acc:    87.50,batch loss: 0.15\nepoch: 133, batch:    23, batch_acc:    100.00,batch loss: 0.05\nepoch: 133, batch:    24, batch_acc:    93.75,batch loss: 0.08\nepoch: 133, batch:    25, batch_acc:    93.75,batch loss: 0.11\nepoch: 133, batch:    26, batch_acc:    100.00,batch loss: 0.03\nepoch: 133, batch:    27, batch_acc:    100.00,batch loss: 0.03\nepoch: 133, batch:    28, batch_acc:    100.00,batch loss: 0.06\nepoch: 133, batch:    29, batch_acc:    100.00,batch loss: 0.06\nepoch: 133, batch:    30, batch_acc:    87.50,batch loss: 0.16\nepoch: 133, batch:    31, batch_acc:    93.75,batch loss: 0.08\nepoch: 133, batch:    32, batch_acc:    93.75,batch loss: 0.08\n 27%|██▋       | 134/500 [00:13<00:37,  9.86it/s]epoch: 134, batch:     1, batch_acc:    87.50,batch loss: 0.15\nepoch: 134, batch:     2, batch_acc:    100.00,batch loss: 0.02\nepoch: 134, batch:     3, batch_acc:    93.75,batch loss: 0.08\nepoch: 134, batch:     4, batch_acc:    100.00,batch loss: 0.06\nepoch: 134, batch:     5, batch_acc:    81.25,batch loss: 0.26\nepoch: 134, batch:     6, batch_acc:    93.75,batch loss: 0.10\nepoch: 134, batch:     7, batch_acc:    87.50,batch loss: 0.19\nepoch: 134, batch:     8, batch_acc:    93.75,batch loss: 0.09\nepoch: 134, batch:     9, batch_acc:    100.00,batch loss: 0.09\nepoch: 134, batch:    10, batch_acc:    93.75,batch loss: 0.13\nepoch: 134, batch:    11, batch_acc:    93.75,batch loss: 0.08\nepoch: 134, batch:    12, batch_acc:    100.00,batch loss: 0.03\nepoch: 134, batch:    13, batch_acc:    93.75,batch loss: 0.12\nepoch: 134, batch:    14, batch_acc:    93.75,batch loss: 0.13\nepoch: 134, batch:    15, batch_acc:    93.75,batch loss: 0.09\nepoch: 134, batch:    16, batch_acc:    93.75,batch loss: 0.18\nepoch: 134, batch:    17, batch_acc:    100.00,batch loss: 0.06\nepoch: 134, batch:    18, batch_acc:    93.75,batch loss: 0.09\nepoch: 134, batch:    19, batch_acc:    75.00,batch loss: 0.29\nepoch: 134, batch:    20, batch_acc:    100.00,batch loss: 0.05\nepoch: 134, batch:    21, batch_acc:    93.75,batch loss: 0.09\nepoch: 134, batch:    22, batch_acc:    87.50,batch loss: 0.15\nepoch: 134, batch:    23, batch_acc:    100.00,batch loss: 0.05\nepoch: 134, batch:    24, batch_acc:    93.75,batch loss: 0.07\nepoch: 134, batch:    25, batch_acc:    93.75,batch loss: 0.10\nepoch: 134, batch:    26, batch_acc:    100.00,batch loss: 0.03\nepoch: 134, batch:    27, batch_acc:    100.00,batch loss: 0.03\nepoch: 134, batch:    28, batch_acc:    100.00,batch loss: 0.06\nepoch: 134, batch:    29, batch_acc:    100.00,batch loss: 0.06\nepoch: 134, batch:    30, batch_acc:    87.50,batch loss: 0.15\nepoch: 134, batch:    31, batch_acc:    100.00,batch loss: 0.08\nepoch: 134, batch:    32, batch_acc:    93.75,batch loss: 0.08\nepoch: 135, batch:     1, batch_acc:    87.50,batch loss: 0.14\nepoch: 135, batch:     2, batch_acc:    100.00,batch loss: 0.02\nepoch: 135, batch:     3, batch_acc:    93.75,batch loss: 0.08\nepoch: 135, batch:     4, batch_acc:    100.00,batch loss: 0.06\nepoch: 135, batch:     5, batch_acc:    81.25,batch loss: 0.26\n"
    }
   ],
   "source": [
    "\n",
    "X,Y = generate_data()\n",
    "layers =[\n",
    "    {'type':'linear','shape':(2,6)},\n",
    "    {'type':'tanh'},\n",
    "    {'type':'linear','shape':(6,6)},\n",
    "    {'type':'tanh'},\n",
    "    {'type':'linear','shape':(6,1)},\n",
    "    {'type':'tanh'}\n",
    "]\n",
    "net = Net(layers)\n",
    "opt = SGD(net.parameters,lr=1e-3)\n",
    "net.train(X,Y,opt)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "X,Y = generate_data()\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=2, activation='tanh'))\n",
    "model.add(Dense(4, activation='tanh'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='MSE',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X, Y,\n",
    "          epochs=200,\n",
    "          batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.7-candidate"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}